{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Influence.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Build MNIST-1D"
      ],
      "metadata": {
        "id": "GGtPsRCh7aLj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEvky5dJ-6JK",
        "outputId": "f5a8853c-fd26-48aa-f64f-dd9c6d919942"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mnist1d'...\n",
            "remote: Enumerating objects: 148, done.\u001b[K\n",
            "remote: Counting objects: 100% (148/148), done.\u001b[K\n",
            "remote: Compressing objects: 100% (122/122), done.\u001b[K\n",
            "remote: Total 148 (delta 49), reused 124 (delta 26), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (148/148), 6.47 MiB | 15.21 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n"
          ]
        }
      ],
      "source": [
        "# Run this if you're in a Colab\n",
        "!git clone https://github.com/greydanus/mnist1d"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pylab as pl\n",
        "import time, copy\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "from scipy.interpolate import interp1d\n",
        "import tensorflow as tf\n",
        "\n",
        "from mnist1d.data import get_templates, get_dataset_args, get_dataset\n",
        "from mnist1d.train import get_model_args, train_model\n",
        "from mnist1d.models import ConvBase, GRUBase, MLPBase, LinearBase\n",
        "from mnist1d.utils import set_seed, plot_signals, ObjectView, from_pickle\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "project_dir = \"/content/gdrive/My Drive/Research/metalearn_afunc/\"\n",
        "\n",
        "PROJECT_DIR = './'\n",
        "\n",
        "class ObjectView(object):\n",
        "    def __init__(self, d): self.__dict__ = d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIoDwONhAIXq",
        "outputId": "4403a7c5-5f91-460a-883f-9dfd403eabb3"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attaching GPU if any"
      ],
      "metadata": {
        "id": "199UGGlT_88u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try attaching to GPU\n",
        "DEVICE = str(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "print('Using:', DEVICE) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjLUlaDFACeO",
        "outputId": "64fe5de6-4c58-400e-efb2-4c729f906148"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the MNIST-1D dataset"
      ],
      "metadata": {
        "id": "MHtr0HswAugR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# args = mnist1d.get_dataset_args()\n",
        "# args.num_samples = 10000\n",
        "# data = mnist1d.get_dataset(args=args, download=False, regenerate=True)  # need to regenerate, since baseline dataset only has 4000/1000 samples\n",
        "import sys ; sys.path.append('..')  # useful if you're running locally\n",
        "import mnist1d\n",
        "\n",
        "args = mnist1d.get_dataset_args()\n",
        "data = mnist1d.get_dataset(args, path='./mnist1d_data.pkl', download=True) # This is the default setting\n",
        "\n",
        "print(\"Examples in training set: {}\".format(len(data['y'])))\n",
        "print(\"Examples in test set: {}\".format(len(data['y_test'])))\n",
        "\n",
        "print(\"Length of each input: {}\".format(data['x'].shape[-1]))\n",
        "\n",
        "print(\"Number of classes: {}\".format(len(data['templates']['y'])))\n",
        "\n",
        "train_data_size = len(data['x'])\n",
        "test_data_size = len(data['x_test'])\n",
        "\n",
        "print(train_data_size)\n",
        "print(test_data_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iE4bff1HAuxm",
        "outputId": "4ddb632f-3bd5-49c7-d726-8b06efea17e3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists. Skipping download.\n",
            "Successfully loaded data from ./mnist1d_data.pkl\n",
            "Examples in training set: 8000\n",
            "Examples in test set: 2000\n",
            "Length of each input: 40\n",
            "Number of classes: 10\n",
            "8000\n",
            "2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEUgOmbQA2gu",
        "outputId": "dfa252f6-d81e-4bce-efa1-f3642c722ce4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['x', 'x_test', 'y', 'y_test', 't', 'templates']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TESTER"
      ],
      "metadata": {
        "id": "-hCdVyhnHuzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.random.mtrand import rand\n",
        "  \n",
        "# generate a random subset of indices for the training and test data\n",
        "random_num_generator = np.random.RandomState(args.seed)\n",
        "\n",
        "# Create a list of random seeds\n",
        "seeds_list = np.random.randint(4000, size = 4)\n",
        "\n",
        "seeds_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ak5IvRM6GRtg",
        "outputId": "b1a85bc5-e662-409f-ebdb-e461c8220ebe"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1296, 1329, 3876, 1233])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Variables"
      ],
      "metadata": {
        "id": "JYWZjXMdZxIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_size = len(data['x'])\n",
        "test_data_size = len(data['x_test'])"
      ],
      "metadata": {
        "id": "t9OQpxTzZ1Ej"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize the hyperparameters"
      ],
      "metadata": {
        "id": "Fyn9L8G8Ear1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = 4000\n",
        "m = int( 0.7 * train_data_size)"
      ],
      "metadata": {
        "id": "_3C44x6VBBRF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample t random subsets of [n] of size m: I_1, I_2, I_3..."
      ],
      "metadata": {
        "id": "t70SeH6fExSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.random.mtrand import rand\n",
        "# generate a random subset of indices for the training and test data\n",
        "random_num_generator = np.random.RandomState(15)\n",
        "\n",
        "# Generate subset of random indices of size m from (0,train_data_size) without replacement.\n",
        "random_indices = np.random.choice(train_data_size, size = m, replace = False)\n",
        "\n",
        "print(len(random_indices))\n",
        "\n",
        "train_images = data[\"x\"][random_indices]\n",
        "print(np.shape(train_images))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD5I24apc538",
        "outputId": "65b4da00-98ae-42d4-d549-d61a27f89bbc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5600\n",
            "(5600, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_subsets(dataset, t_iterations, m_ratio):\n",
        "  '''Creates a subset of a dataset with given ratio. Parameter t_iterations - number of iterations / subsets. m_ratio - size of the subset.'''\n",
        "  # Lists that will contain subsets of random samples\n",
        "  # train_random_samples = []\n",
        "  # test_random_samples = []\n",
        "\n",
        "  # Create a subset structure with the same testing data\n",
        "  subset = {'x':None,'y':None, 'x_test':dataset['x_test'],'y_test':dataset['y_test'],'indices':None}\n",
        "\n",
        "  # Create a list of subsets\n",
        "  list_of_subsets = []\n",
        "\n",
        "  # create a random number generator\n",
        "  random_num_generator = np.random.RandomState(args.seed)\n",
        "\n",
        "  # Create a list of random seeds\n",
        "  seeds_list = np.random.randint(t_iterations, size = t_iterations)\n",
        "  \n",
        "  for i in range(t_iterations):\n",
        "    \n",
        "    # create a random number generator\n",
        "    random_num_generator = np.random.RandomState(seeds_list[i])\n",
        "\n",
        "    # Generate subset of random indices of size m from (0,train_data_size) without replacement.\n",
        "    random_indices = random_num_generator.choice(train_data_size, size = m, replace = False)\n",
        "\n",
        "    # Save the random indices in the subset structure\n",
        "    subset['indices'] = random_indices\n",
        "    \n",
        "    # Create a subset of training images and then of testing images / labels\n",
        "    subset['x'] = dataset[\"x\"][random_indices]\n",
        "    subset['y'] = dataset[\"y\"][random_indices]\n",
        "\n",
        "    # Append the subset to the lists of subsets\n",
        "    list_of_subsets.append(subset)\n",
        "\n",
        "  return list_of_subsets\n"
      ],
      "metadata": {
        "id": "c5C79oglE-BI"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_model_args(as_dict=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_5zOo8Xkd2i",
        "outputId": "8b8e6d8e-0be1-466d-efb9-d1a4a1e3b45a"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 100,\n",
              " 'checkpoint_every': 1000,\n",
              " 'device': 'cpu',\n",
              " 'eval_every': 250,\n",
              " 'hidden_size': 256,\n",
              " 'input_size': 40,\n",
              " 'learning_rate': 0.01,\n",
              " 'output_size': 10,\n",
              " 'print_every': 1000,\n",
              " 'seed': 42,\n",
              " 'total_steps': 8000,\n",
              " 'weight_decay': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create all necessary subsets"
      ],
      "metadata": {
        "id": "1cB-04qBoXUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subsets = create_subsets(data,t,m)"
      ],
      "metadata": {
        "id": "llwp2mE7oWuu"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algorithm A"
      ],
      "metadata": {
        "id": "1qTRb2cEEevw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the model info\n",
        "args = get_model_args()\n",
        "\n",
        "# list to keep all the models\n",
        "list_of_mlp_models = []\n",
        "list_of_ConvBase_models = []\n",
        "\n",
        "# list ot keep all the training results\n",
        "trained_mlp_model_results = []\n",
        "trained_ConvBase_model_results = []\n",
        "\n",
        "def train_MLPBase_models(t,subsets,args):\n",
        "  '''Creates t MLPBase models. Parameter t - number of trials.'''\n",
        "  # Create and traing t models\n",
        "  for k in range(t):\n",
        "    # set the seed\n",
        "    set_seed(k)\n",
        "\n",
        "    # create a model\n",
        "    model = MLPBase(args.input_size, args.output_size)\n",
        "    \n",
        "    # append model to the list of models\n",
        "    list_of_mlp_models.append(model)\n",
        "\n",
        "    # define the subset of data you wan to use\n",
        "    data_subset = subsets[k]\n",
        "\n",
        "    # train the model\n",
        "    mlp_training_results = train_model(data_subset, model, args)\n",
        "\n",
        "    # append the results of the model\n",
        "    trained_mlp_model_results.append(mlp_training_results)\n",
        "\n",
        "  return list_of_mlp_models, trained_mlp_model_results\n",
        "\n",
        "def train_ConvBase_models(t,subsets,args):\n",
        "  '''Creates t ConvBase models. Parameter t - number of trials.'''\n",
        "  # Create and traing t models\n",
        "  for k in range(t):\n",
        "    # set the seed\n",
        "    #set_seed(args.seed)\n",
        "    set_seed(k)\n",
        "\n",
        "    # create a model\n",
        "    model = ConvBase(output_size=args.output_size)\n",
        "    \n",
        "    # append model to the list of models\n",
        "    list_of_ConvBase_models.append(model)\n",
        "\n",
        "    # define the subset of data you wan to use\n",
        "    data_subset = subsets[k]\n",
        "\n",
        "    # train the model\n",
        "    ConvBase_training_results = train_model(data_subset, model, args)\n",
        "\n",
        "    # append the results of the model\n",
        "    trained_ConvBase_model_results.append(ConvBase_training_results)\n",
        "\n",
        "  return list_of_ConvBase_models, trained_ConvBase_model_results\n",
        "\n"
      ],
      "metadata": {
        "id": "-AhT1-U6Ed4c"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the models on the subsets"
      ],
      "metadata": {
        "id": "2NT4ngZ5058L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP base\n",
        "list_of_mlp_models, trained_mlp_model_results = train_MLPBase_models(15,subsets,args)\n",
        "\n",
        "# ConvBase\n",
        "list_of_ConvBase_models, trained_ConvBase_model_results = train_ConvBase_models(12,subsets,args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQ1j64OP05do",
        "outputId": "18413b3f-14fe-49b9-eb3d-0356ede26801"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized MLPBase model with 15210 parameters\n",
            "step 1000, dt 1.35s, train_loss 3.476e-01, test_loss 1.225e+00, train_acc 89.5, test_acc 65.8\n",
            "step 2000, dt 1.52s, train_loss 5.535e-02, test_loss 1.891e+00, train_acc 95.1, test_acc 67.5\n",
            "step 3000, dt 1.50s, train_loss 3.627e-02, test_loss 2.393e+00, train_acc 97.5, test_acc 69.1\n",
            "step 4000, dt 1.43s, train_loss 3.986e-02, test_loss 2.928e+00, train_acc 95.9, test_acc 68.3\n",
            "step 5000, dt 1.60s, train_loss 6.464e-02, test_loss 3.416e+00, train_acc 98.9, test_acc 68.3\n",
            "step 6000, dt 1.67s, train_loss 1.791e-01, test_loss 3.817e+00, train_acc 98.3, test_acc 68.1\n",
            "step 7000, dt 1.77s, train_loss 4.729e-02, test_loss 4.617e+00, train_acc 98.2, test_acc 68.0\n",
            "step 8000, dt 1.59s, train_loss 1.003e-01, test_loss 4.906e+00, train_acc 97.5, test_acc 67.9\n",
            "Initialized MLPBase model with 15210 parameters\n",
            "step 1000, dt 1.38s, train_loss 3.679e-01, test_loss 1.495e+00, train_acc 82.4, test_acc 62.4\n",
            "step 2000, dt 1.34s, train_loss 1.481e-01, test_loss 2.175e+00, train_acc 92.2, test_acc 63.3\n",
            "step 3000, dt 1.35s, train_loss 6.360e-02, test_loss 2.555e+00, train_acc 95.8, test_acc 67.8\n",
            "step 4000, dt 1.55s, train_loss 1.105e-01, test_loss 3.070e+00, train_acc 97.8, test_acc 67.0\n",
            "step 5000, dt 1.59s, train_loss 5.941e-02, test_loss 3.493e+00, train_acc 98.6, test_acc 67.2\n",
            "step 6000, dt 1.52s, train_loss 3.940e-02, test_loss 3.919e+00, train_acc 99.2, test_acc 66.3\n",
            "step 7000, dt 1.61s, train_loss 1.722e-01, test_loss 4.992e+00, train_acc 97.0, test_acc 65.8\n",
            "step 8000, dt 1.53s, train_loss 8.921e-02, test_loss 4.921e+00, train_acc 98.2, test_acc 67.2\n",
            "Initialized MLPBase model with 15210 parameters\n",
            "step 1000, dt 1.29s, train_loss 3.248e-01, test_loss 1.406e+00, train_acc 87.6, test_acc 64.2\n",
            "step 2000, dt 1.40s, train_loss 1.920e-01, test_loss 2.227e+00, train_acc 92.8, test_acc 64.2\n",
            "step 3000, dt 1.43s, train_loss 2.356e-01, test_loss 2.676e+00, train_acc 96.7, test_acc 66.3\n",
            "step 4000, dt 1.42s, train_loss 4.689e-02, test_loss 3.267e+00, train_acc 96.6, test_acc 66.0\n",
            "step 5000, dt 1.75s, train_loss 2.337e-01, test_loss 3.629e+00, train_acc 98.6, test_acc 66.5\n",
            "step 6000, dt 1.67s, train_loss 4.648e-02, test_loss 4.460e+00, train_acc 97.5, test_acc 65.8\n",
            "step 7000, dt 1.65s, train_loss 3.567e-01, test_loss 5.079e+00, train_acc 96.5, test_acc 65.8\n",
            "step 8000, dt 1.67s, train_loss 2.123e-01, test_loss 5.405e+00, train_acc 97.6, test_acc 67.4\n",
            "Initialized MLPBase model with 15210 parameters\n",
            "step 1000, dt 1.38s, train_loss 4.585e-01, test_loss 1.291e+00, train_acc 86.3, test_acc 66.8\n",
            "step 2000, dt 1.33s, train_loss 2.591e-01, test_loss 2.187e+00, train_acc 90.3, test_acc 65.3\n",
            "step 3000, dt 1.46s, train_loss 1.486e-01, test_loss 2.678e+00, train_acc 93.1, test_acc 65.8\n",
            "step 4000, dt 1.43s, train_loss 1.545e-01, test_loss 3.016e+00, train_acc 97.6, test_acc 67.7\n",
            "step 5000, dt 1.42s, train_loss 2.870e-02, test_loss 3.501e+00, train_acc 98.3, test_acc 67.8\n",
            "step 6000, dt 1.55s, train_loss 8.617e-02, test_loss 3.886e+00, train_acc 99.1, test_acc 66.8\n",
            "step 7000, dt 2.27s, train_loss 5.164e-02, test_loss 4.365e+00, train_acc 97.4, test_acc 66.3\n",
            "step 8000, dt 1.67s, train_loss 1.805e-04, test_loss 4.072e+00, train_acc 100.0, test_acc 68.5\n",
            "Initialized MLPBase model with 15210 parameters\n",
            "step 1000, dt 1.30s, train_loss 3.769e-01, test_loss 1.436e+00, train_acc 84.2, test_acc 62.9\n",
            "step 2000, dt 1.38s, train_loss 9.931e-02, test_loss 2.157e+00, train_acc 93.0, test_acc 64.5\n",
            "step 3000, dt 1.36s, train_loss 4.755e-02, test_loss 2.639e+00, train_acc 96.9, test_acc 66.8\n",
            "step 4000, dt 2.04s, train_loss 8.547e-02, test_loss 3.065e+00, train_acc 96.7, test_acc 67.1\n",
            "step 5000, dt 1.71s, train_loss 1.576e-01, test_loss 3.482e+00, train_acc 98.6, test_acc 67.3\n",
            "step 6000, dt 1.62s, train_loss 6.808e-02, test_loss 4.004e+00, train_acc 97.9, test_acc 66.5\n",
            "step 7000, dt 1.65s, train_loss 9.986e-03, test_loss 4.359e+00, train_acc 99.4, test_acc 68.8\n",
            "step 8000, dt 1.65s, train_loss 4.144e-02, test_loss 4.781e+00, train_acc 98.7, test_acc 66.8\n",
            "Initialized MLPBase model with 15210 parameters\n",
            "step 1000, dt 1.29s, train_loss 6.151e-01, test_loss 1.340e+00, train_acc 83.7, test_acc 64.0\n",
            "step 2000, dt 1.40s, train_loss 4.130e-01, test_loss 1.887e+00, train_acc 93.6, test_acc 65.7\n",
            "step 3000, dt 1.34s, train_loss 1.811e-01, test_loss 2.692e+00, train_acc 95.1, test_acc 67.3\n",
            "step 4000, dt 1.38s, train_loss 1.028e-01, test_loss 2.861e+00, train_acc 98.7, test_acc 69.2\n",
            "step 5000, dt 1.51s, train_loss 2.916e-02, test_loss 3.535e+00, train_acc 99.1, test_acc 67.8\n",
            "step 6000, dt 1.48s, train_loss 1.724e-02, test_loss 3.673e+00, train_acc 99.3, test_acc 67.2\n",
            "step 7000, dt 1.66s, train_loss 8.371e-03, test_loss 4.181e+00, train_acc 99.3, test_acc 67.6\n",
            "step 8000, dt 1.63s, train_loss 3.125e-03, test_loss 4.633e+00, train_acc 99.5, test_acc 67.6\n",
            "Initialized MLPBase model with 15210 parameters\n",
            "step 1000, dt 1.26s, train_loss 3.342e-01, test_loss 1.314e+00, train_acc 87.4, test_acc 65.8\n",
            "step 2000, dt 1.27s, train_loss 8.394e-02, test_loss 2.051e+00, train_acc 94.5, test_acc 66.7\n",
            "step 3000, dt 1.31s, train_loss 2.182e-02, test_loss 2.584e+00, train_acc 96.8, test_acc 68.4\n",
            "step 4000, dt 1.48s, train_loss 1.710e-02, test_loss 3.067e+00, train_acc 97.5, test_acc 67.6\n",
            "step 5000, dt 1.44s, train_loss 9.206e-02, test_loss 3.405e+00, train_acc 96.9, test_acc 68.8\n",
            "step 6000, dt 1.61s, train_loss 1.676e-01, test_loss 4.053e+00, train_acc 98.0, test_acc 68.5\n",
            "step 7000, dt 1.61s, train_loss 3.773e-01, test_loss 4.901e+00, train_acc 95.5, test_acc 66.8\n",
            "step 8000, dt 1.52s, train_loss 8.481e-02, test_loss 5.253e+00, train_acc 97.6, test_acc 67.8\n",
            "Initialized MLPBase model with 15210 parameters\n",
            "step 1000, dt 1.30s, train_loss 3.351e-01, test_loss 1.325e+00, train_acc 86.1, test_acc 64.3\n",
            "step 2000, dt 1.55s, train_loss 3.056e-01, test_loss 2.026e+00, train_acc 92.7, test_acc 65.5\n",
            "step 3000, dt 1.44s, train_loss 9.659e-02, test_loss 2.422e+00, train_acc 97.2, test_acc 67.8\n",
            "step 4000, dt 1.37s, train_loss 1.679e-01, test_loss 3.039e+00, train_acc 96.4, test_acc 67.5\n",
            "step 5000, dt 1.59s, train_loss 2.565e-02, test_loss 3.295e+00, train_acc 99.3, test_acc 67.5\n",
            "step 6000, dt 1.61s, train_loss 1.589e-03, test_loss 3.478e+00, train_acc 99.8, test_acc 69.2\n",
            "step 7000, dt 1.58s, train_loss 2.408e-04, test_loss 3.560e+00, train_acc 100.0, test_acc 68.5\n",
            "step 8000, dt 1.67s, train_loss 1.474e-04, test_loss 3.611e+00, train_acc 100.0, test_acc 68.7\n",
            "Initialized MLPBase model with 15210 parameters\n",
            "step 1000, dt 1.32s, train_loss 3.159e-01, test_loss 1.345e+00, train_acc 84.9, test_acc 64.3\n",
            "step 2000, dt 1.46s, train_loss 2.164e-01, test_loss 2.020e+00, train_acc 92.4, test_acc 66.8\n",
            "step 3000, dt 1.40s, train_loss 1.971e-01, test_loss 2.592e+00, train_acc 94.1, test_acc 66.9\n",
            "step 4000, dt 1.54s, train_loss 1.661e-01, test_loss 2.811e+00, train_acc 98.8, test_acc 68.8\n",
            "step 5000, dt 1.44s, train_loss 4.523e-02, test_loss 3.300e+00, train_acc 99.1, test_acc 68.9\n",
            "step 6000, dt 1.62s, train_loss 1.422e-02, test_loss 3.729e+00, train_acc 99.1, test_acc 68.1\n",
            "step 7000, dt 1.65s, train_loss 3.514e-02, test_loss 4.095e+00, train_acc 98.7, test_acc 67.6\n",
            "step 8000, dt 1.86s, train_loss 7.339e-03, test_loss 4.464e+00, train_acc 98.8, test_acc 69.2\n",
            "Initialized MLPBase model with 15210 parameters\n",
            "step 1000, dt 1.70s, train_loss 4.759e-01, test_loss 1.387e+00, train_acc 86.4, test_acc 64.0\n",
            "step 2000, dt 1.89s, train_loss 1.270e-01, test_loss 2.183e+00, train_acc 91.8, test_acc 63.9\n",
            "step 3000, dt 1.28s, train_loss 1.776e-01, test_loss 2.585e+00, train_acc 95.4, test_acc 67.7\n",
            "step 4000, dt 1.66s, train_loss 2.070e-01, test_loss 3.143e+00, train_acc 96.8, test_acc 66.4\n",
            "step 5000, dt 1.55s, train_loss 3.831e-02, test_loss 3.691e+00, train_acc 97.9, test_acc 65.7\n",
            "step 6000, dt 1.54s, train_loss 2.614e-03, test_loss 3.773e+00, train_acc 99.9, test_acc 68.7\n",
            "step 7000, dt 1.61s, train_loss 4.684e-03, test_loss 4.362e+00, train_acc 99.1, test_acc 67.7\n",
            "step 8000, dt 1.66s, train_loss 7.055e-02, test_loss 4.808e+00, train_acc 99.1, test_acc 67.2\n",
            "Initialized MLPBase model with 15210 parameters\n",
            "step 1000, dt 1.28s, train_loss 4.710e-01, test_loss 1.204e+00, train_acc 85.3, test_acc 64.5\n",
            "step 2000, dt 1.46s, train_loss 2.218e-01, test_loss 2.107e+00, train_acc 91.1, test_acc 64.8\n",
            "step 3000, dt 1.39s, train_loss 2.212e-01, test_loss 2.599e+00, train_acc 94.8, test_acc 65.8\n",
            "step 4000, dt 1.55s, train_loss 1.566e-01, test_loss 3.430e+00, train_acc 94.9, test_acc 65.6\n",
            "step 5000, dt 1.63s, train_loss 1.566e-01, test_loss 3.719e+00, train_acc 97.9, test_acc 67.0\n",
            "step 6000, dt 1.51s, train_loss 1.615e-02, test_loss 3.795e+00, train_acc 99.2, test_acc 67.3\n",
            "step 7000, dt 1.64s, train_loss 1.293e-02, test_loss 4.141e+00, train_acc 99.9, test_acc 68.2\n",
            "step 8000, dt 1.58s, train_loss 1.153e-04, test_loss 4.059e+00, train_acc 100.0, test_acc 68.0\n",
            "Initialized MLPBase model with 15210 parameters\n",
            "step 1000, dt 1.49s, train_loss 3.526e-01, test_loss 1.348e+00, train_acc 86.6, test_acc 66.0\n",
            "step 2000, dt 1.32s, train_loss 1.427e-01, test_loss 2.133e+00, train_acc 92.7, test_acc 64.8\n",
            "step 3000, dt 1.47s, train_loss 1.052e-01, test_loss 2.509e+00, train_acc 95.8, test_acc 68.4\n",
            "step 4000, dt 1.41s, train_loss 1.763e-01, test_loss 3.001e+00, train_acc 98.0, test_acc 68.4\n",
            "step 5000, dt 1.56s, train_loss 1.921e-01, test_loss 3.594e+00, train_acc 98.0, test_acc 68.3\n",
            "step 6000, dt 1.52s, train_loss 9.192e-04, test_loss 4.174e+00, train_acc 99.2, test_acc 68.0\n",
            "step 7000, dt 1.74s, train_loss 2.379e-02, test_loss 4.673e+00, train_acc 99.1, test_acc 67.5\n",
            "step 8000, dt 3.95s, train_loss 1.175e-01, test_loss 4.893e+00, train_acc 99.6, test_acc 69.7\n",
            "Initialized MLPBase model with 15210 parameters\n",
            "step 1000, dt 2.98s, train_loss 3.519e-01, test_loss 1.234e+00, train_acc 86.4, test_acc 66.8\n",
            "step 2000, dt 1.52s, train_loss 1.696e-01, test_loss 1.911e+00, train_acc 91.6, test_acc 66.8\n",
            "step 3000, dt 1.36s, train_loss 8.611e-02, test_loss 2.494e+00, train_acc 95.6, test_acc 67.2\n",
            "step 4000, dt 1.52s, train_loss 1.571e-01, test_loss 2.763e+00, train_acc 98.4, test_acc 67.6\n",
            "step 5000, dt 1.58s, train_loss 5.869e-03, test_loss 3.154e+00, train_acc 99.4, test_acc 69.0\n",
            "step 6000, dt 1.64s, train_loss 3.058e-04, test_loss 3.161e+00, train_acc 100.0, test_acc 70.4\n",
            "step 7000, dt 1.49s, train_loss 1.896e-04, test_loss 3.216e+00, train_acc 100.0, test_acc 70.3\n",
            "step 8000, dt 1.56s, train_loss 7.318e-05, test_loss 3.273e+00, train_acc 100.0, test_acc 70.3\n",
            "Initialized MLPBase model with 15210 parameters\n",
            "step 1000, dt 1.36s, train_loss 3.320e-01, test_loss 1.388e+00, train_acc 86.4, test_acc 63.8\n",
            "step 2000, dt 1.34s, train_loss 2.374e-01, test_loss 2.036e+00, train_acc 93.9, test_acc 65.2\n",
            "step 3000, dt 1.48s, train_loss 1.255e-01, test_loss 2.791e+00, train_acc 95.3, test_acc 66.2\n",
            "step 4000, dt 1.38s, train_loss 9.395e-02, test_loss 3.188e+00, train_acc 97.6, test_acc 66.1\n",
            "step 5000, dt 1.97s, train_loss 1.384e-01, test_loss 3.637e+00, train_acc 97.2, test_acc 66.5\n",
            "step 6000, dt 4.22s, train_loss 1.413e-01, test_loss 4.247e+00, train_acc 97.4, test_acc 67.0\n",
            "step 7000, dt 1.63s, train_loss 1.723e-02, test_loss 4.489e+00, train_acc 98.9, test_acc 67.7\n",
            "step 8000, dt 1.74s, train_loss 3.320e-03, test_loss 5.136e+00, train_acc 97.0, test_acc 67.0\n",
            "Initialized MLPBase model with 15210 parameters\n",
            "step 1000, dt 1.31s, train_loss 3.981e-01, test_loss 1.351e+00, train_acc 84.7, test_acc 63.9\n",
            "step 2000, dt 1.39s, train_loss 2.346e-01, test_loss 1.934e+00, train_acc 92.7, test_acc 65.9\n",
            "step 3000, dt 2.38s, train_loss 2.763e-02, test_loss 2.362e+00, train_acc 95.8, test_acc 68.3\n",
            "step 4000, dt 1.52s, train_loss 5.672e-01, test_loss 3.138e+00, train_acc 92.9, test_acc 65.8\n",
            "step 5000, dt 1.58s, train_loss 1.947e-01, test_loss 3.201e+00, train_acc 96.6, test_acc 67.9\n",
            "step 6000, dt 2.38s, train_loss 1.491e-01, test_loss 3.895e+00, train_acc 97.7, test_acc 68.0\n",
            "step 7000, dt 2.30s, train_loss 3.745e-02, test_loss 4.045e+00, train_acc 98.7, test_acc 68.0\n",
            "step 8000, dt 1.69s, train_loss 2.451e-01, test_loss 4.634e+00, train_acc 97.7, test_acc 67.2\n",
            "Initialized ConvBase model with 5210 parameters\n",
            "step 1000, dt 5.52s, train_loss 5.579e-02, test_loss 3.482e-01, train_acc 95.9, test_acc 89.7\n",
            "step 2000, dt 4.95s, train_loss 1.295e-02, test_loss 2.836e-01, train_acc 98.5, test_acc 93.8\n",
            "step 3000, dt 4.43s, train_loss 8.018e-02, test_loss 4.049e-01, train_acc 97.2, test_acc 93.2\n",
            "step 4000, dt 6.06s, train_loss 8.013e-03, test_loss 3.181e-01, train_acc 99.5, test_acc 94.5\n",
            "step 5000, dt 4.68s, train_loss 8.172e-02, test_loss 5.076e-01, train_acc 98.8, test_acc 93.0\n",
            "step 6000, dt 5.11s, train_loss 9.659e-02, test_loss 5.227e-01, train_acc 98.3, test_acc 93.4\n",
            "step 7000, dt 5.39s, train_loss 6.333e-05, test_loss 3.204e-01, train_acc 100.0, test_acc 95.8\n",
            "step 8000, dt 5.59s, train_loss 1.578e-05, test_loss 3.363e-01, train_acc 100.0, test_acc 95.8\n",
            "Initialized ConvBase model with 5210 parameters\n",
            "step 1000, dt 4.30s, train_loss 4.431e-02, test_loss 3.262e-01, train_acc 95.7, test_acc 90.0\n",
            "step 2000, dt 4.28s, train_loss 2.473e-02, test_loss 3.663e-01, train_acc 98.2, test_acc 92.0\n",
            "step 3000, dt 4.48s, train_loss 3.196e-02, test_loss 4.733e-01, train_acc 97.3, test_acc 91.9\n",
            "step 4000, dt 4.51s, train_loss 3.114e-04, test_loss 3.248e-01, train_acc 100.0, test_acc 95.0\n",
            "step 5000, dt 4.46s, train_loss 1.224e-04, test_loss 3.518e-01, train_acc 100.0, test_acc 95.0\n",
            "step 6000, dt 4.64s, train_loss 5.719e-05, test_loss 3.713e-01, train_acc 100.0, test_acc 94.9\n",
            "step 7000, dt 6.42s, train_loss 4.106e-05, test_loss 3.895e-01, train_acc 100.0, test_acc 95.0\n",
            "step 8000, dt 4.83s, train_loss 8.786e-06, test_loss 4.057e-01, train_acc 100.0, test_acc 94.8\n",
            "Initialized ConvBase model with 5210 parameters\n",
            "step 1000, dt 4.24s, train_loss 8.723e-02, test_loss 3.015e-01, train_acc 97.3, test_acc 91.5\n",
            "step 2000, dt 4.19s, train_loss 8.447e-02, test_loss 4.355e-01, train_acc 95.9, test_acc 90.3\n",
            "step 3000, dt 4.46s, train_loss 1.174e-01, test_loss 3.949e-01, train_acc 98.6, test_acc 93.2\n",
            "step 4000, dt 5.17s, train_loss 3.355e-02, test_loss 4.159e-01, train_acc 98.4, test_acc 93.2\n",
            "step 5000, dt 5.11s, train_loss 8.595e-02, test_loss 5.342e-01, train_acc 98.8, test_acc 93.5\n",
            "step 6000, dt 5.83s, train_loss 4.163e-02, test_loss 4.995e-01, train_acc 99.0, test_acc 93.8\n",
            "step 7000, dt 5.79s, train_loss 4.379e-03, test_loss 5.186e-01, train_acc 99.3, test_acc 94.5\n",
            "step 8000, dt 5.72s, train_loss 3.295e-03, test_loss 4.549e-01, train_acc 99.7, test_acc 95.0\n",
            "Initialized ConvBase model with 5210 parameters\n",
            "step 1000, dt 4.43s, train_loss 1.839e-01, test_loss 3.591e-01, train_acc 94.2, test_acc 89.1\n",
            "step 2000, dt 5.19s, train_loss 8.491e-02, test_loss 2.999e-01, train_acc 98.8, test_acc 93.2\n",
            "step 3000, dt 4.33s, train_loss 8.281e-03, test_loss 3.375e-01, train_acc 98.8, test_acc 93.4\n",
            "step 4000, dt 4.46s, train_loss 2.780e-02, test_loss 4.507e-01, train_acc 99.0, test_acc 92.7\n",
            "step 5000, dt 4.73s, train_loss 2.935e-03, test_loss 3.799e-01, train_acc 99.8, test_acc 95.2\n",
            "step 6000, dt 5.05s, train_loss 1.039e-04, test_loss 3.555e-01, train_acc 100.0, test_acc 95.5\n",
            "step 7000, dt 5.17s, train_loss 5.999e-05, test_loss 3.655e-01, train_acc 100.0, test_acc 95.3\n",
            "step 8000, dt 5.25s, train_loss 1.757e-05, test_loss 3.758e-01, train_acc 100.0, test_acc 95.2\n",
            "Initialized ConvBase model with 5210 parameters\n",
            "step 1000, dt 4.22s, train_loss 1.020e-01, test_loss 2.608e-01, train_acc 97.3, test_acc 92.2\n",
            "step 2000, dt 4.20s, train_loss 8.366e-03, test_loss 2.782e-01, train_acc 98.9, test_acc 93.4\n",
            "step 3000, dt 4.24s, train_loss 5.552e-02, test_loss 3.852e-01, train_acc 98.9, test_acc 93.5\n",
            "step 4000, dt 4.63s, train_loss 4.290e-02, test_loss 4.733e-01, train_acc 98.1, test_acc 93.0\n",
            "step 5000, dt 4.91s, train_loss 4.098e-04, test_loss 3.598e-01, train_acc 99.2, test_acc 94.7\n",
            "step 6000, dt 5.44s, train_loss 2.115e-02, test_loss 5.487e-01, train_acc 98.5, test_acc 94.1\n",
            "step 7000, dt 5.50s, train_loss 6.463e-02, test_loss 4.567e-01, train_acc 99.3, test_acc 94.5\n",
            "step 8000, dt 5.91s, train_loss 1.399e-01, test_loss 4.857e-01, train_acc 99.2, test_acc 95.9\n",
            "Initialized ConvBase model with 5210 parameters\n",
            "step 1000, dt 4.26s, train_loss 1.559e-01, test_loss 3.652e-01, train_acc 93.8, test_acc 89.0\n",
            "step 2000, dt 4.23s, train_loss 1.412e-01, test_loss 3.914e-01, train_acc 97.8, test_acc 91.3\n",
            "step 3000, dt 4.58s, train_loss 2.219e-02, test_loss 4.406e-01, train_acc 98.4, test_acc 92.0\n",
            "step 4000, dt 4.69s, train_loss 8.511e-02, test_loss 5.101e-01, train_acc 97.6, test_acc 92.0\n",
            "step 5000, dt 4.82s, train_loss 6.442e-02, test_loss 5.042e-01, train_acc 99.0, test_acc 91.9\n",
            "step 6000, dt 5.27s, train_loss 1.226e-01, test_loss 5.767e-01, train_acc 98.2, test_acc 93.3\n",
            "step 7000, dt 5.64s, train_loss 2.310e-02, test_loss 6.354e-01, train_acc 99.4, test_acc 93.1\n",
            "step 8000, dt 5.54s, train_loss 3.170e-04, test_loss 5.922e-01, train_acc 99.8, test_acc 92.8\n",
            "Initialized ConvBase model with 5210 parameters\n",
            "step 1000, dt 4.32s, train_loss 1.123e-01, test_loss 3.456e-01, train_acc 96.4, test_acc 90.0\n",
            "step 2000, dt 4.06s, train_loss 4.267e-02, test_loss 3.327e-01, train_acc 97.9, test_acc 92.6\n",
            "step 3000, dt 4.37s, train_loss 2.628e-02, test_loss 4.050e-01, train_acc 98.8, test_acc 92.2\n",
            "step 4000, dt 5.04s, train_loss 1.305e-01, test_loss 4.403e-01, train_acc 98.2, test_acc 92.3\n",
            "step 5000, dt 4.84s, train_loss 1.271e-03, test_loss 4.675e-01, train_acc 98.9, test_acc 93.0\n",
            "step 6000, dt 5.23s, train_loss 1.164e-01, test_loss 5.912e-01, train_acc 98.1, test_acc 92.5\n",
            "step 7000, dt 5.34s, train_loss 6.407e-04, test_loss 3.889e-01, train_acc 99.9, test_acc 95.2\n",
            "step 8000, dt 5.66s, train_loss 4.213e-02, test_loss 4.739e-01, train_acc 98.7, test_acc 93.2\n",
            "Initialized ConvBase model with 5210 parameters\n",
            "step 1000, dt 4.23s, train_loss 2.555e-01, test_loss 2.875e-01, train_acc 96.0, test_acc 91.2\n",
            "step 2000, dt 4.13s, train_loss 1.275e-01, test_loss 3.513e-01, train_acc 97.9, test_acc 91.6\n",
            "step 3000, dt 4.25s, train_loss 2.512e-03, test_loss 4.503e-01, train_acc 98.9, test_acc 92.0\n",
            "step 4000, dt 4.71s, train_loss 1.269e-02, test_loss 2.957e-01, train_acc 98.8, test_acc 94.5\n",
            "step 5000, dt 5.08s, train_loss 1.350e-02, test_loss 4.703e-01, train_acc 98.9, test_acc 94.1\n",
            "step 6000, dt 5.59s, train_loss 7.822e-05, test_loss 4.377e-01, train_acc 99.6, test_acc 95.3\n",
            "step 7000, dt 5.80s, train_loss 1.030e-04, test_loss 3.702e-01, train_acc 100.0, test_acc 95.0\n",
            "step 8000, dt 5.95s, train_loss 5.320e-06, test_loss 3.769e-01, train_acc 100.0, test_acc 95.2\n",
            "Initialized ConvBase model with 5210 parameters\n",
            "step 1000, dt 4.16s, train_loss 5.720e-02, test_loss 3.057e-01, train_acc 96.9, test_acc 91.2\n",
            "step 2000, dt 4.20s, train_loss 1.203e-02, test_loss 2.634e-01, train_acc 99.5, test_acc 94.0\n",
            "step 3000, dt 4.40s, train_loss 4.164e-02, test_loss 4.628e-01, train_acc 99.3, test_acc 92.7\n",
            "step 4000, dt 4.37s, train_loss 3.567e-02, test_loss 4.020e-01, train_acc 98.9, test_acc 92.8\n",
            "step 5000, dt 5.15s, train_loss 6.079e-02, test_loss 4.196e-01, train_acc 99.4, test_acc 94.0\n",
            "step 6000, dt 5.00s, train_loss 4.841e-04, test_loss 4.485e-01, train_acc 99.9, test_acc 94.8\n",
            "step 7000, dt 7.20s, train_loss 3.304e-05, test_loss 4.560e-01, train_acc 99.8, test_acc 95.0\n",
            "step 8000, dt 9.19s, train_loss 5.159e-02, test_loss 6.501e-01, train_acc 99.6, test_acc 94.1\n",
            "Initialized ConvBase model with 5210 parameters\n",
            "step 1000, dt 4.91s, train_loss 6.148e-02, test_loss 3.225e-01, train_acc 96.5, test_acc 91.0\n",
            "step 2000, dt 4.95s, train_loss 7.381e-02, test_loss 3.750e-01, train_acc 98.8, test_acc 92.9\n",
            "step 3000, dt 4.90s, train_loss 1.396e-02, test_loss 4.494e-01, train_acc 98.4, test_acc 92.4\n",
            "step 4000, dt 4.99s, train_loss 5.053e-02, test_loss 3.575e-01, train_acc 99.5, test_acc 93.8\n",
            "step 5000, dt 5.51s, train_loss 1.002e-02, test_loss 5.345e-01, train_acc 98.9, test_acc 93.8\n",
            "step 6000, dt 5.68s, train_loss 1.375e-03, test_loss 3.977e-01, train_acc 99.9, test_acc 95.4\n",
            "step 7000, dt 6.06s, train_loss 2.536e-01, test_loss 5.767e-01, train_acc 97.9, test_acc 93.2\n",
            "step 8000, dt 6.59s, train_loss 2.452e-02, test_loss 5.706e-01, train_acc 99.1, test_acc 94.8\n",
            "Initialized ConvBase model with 5210 parameters\n",
            "step 1000, dt 6.29s, train_loss 7.772e-02, test_loss 3.470e-01, train_acc 94.6, test_acc 89.8\n",
            "step 2000, dt 6.78s, train_loss 5.857e-02, test_loss 3.448e-01, train_acc 98.5, test_acc 93.0\n",
            "step 3000, dt 6.40s, train_loss 5.164e-02, test_loss 4.609e-01, train_acc 98.8, test_acc 92.5\n",
            "step 4000, dt 5.83s, train_loss 1.330e-01, test_loss 4.614e-01, train_acc 99.2, test_acc 93.7\n",
            "step 5000, dt 7.30s, train_loss 7.384e-03, test_loss 5.498e-01, train_acc 98.7, test_acc 93.1\n",
            "step 6000, dt 7.96s, train_loss 7.680e-04, test_loss 4.384e-01, train_acc 99.6, test_acc 94.2\n",
            "step 7000, dt 6.08s, train_loss 1.710e-03, test_loss 6.212e-01, train_acc 98.7, test_acc 93.0\n",
            "step 8000, dt 6.75s, train_loss 9.328e-02, test_loss 6.362e-01, train_acc 99.4, test_acc 94.6\n",
            "Initialized ConvBase model with 5210 parameters\n",
            "step 1000, dt 6.27s, train_loss 5.163e-02, test_loss 3.525e-01, train_acc 95.3, test_acc 89.6\n",
            "step 2000, dt 4.63s, train_loss 1.207e-01, test_loss 3.030e-01, train_acc 97.2, test_acc 92.6\n",
            "step 3000, dt 4.59s, train_loss 6.167e-03, test_loss 4.202e-01, train_acc 98.8, test_acc 92.5\n",
            "step 4000, dt 5.81s, train_loss 3.140e-02, test_loss 3.773e-01, train_acc 99.4, test_acc 94.2\n",
            "step 5000, dt 5.41s, train_loss 1.717e-03, test_loss 4.971e-01, train_acc 98.9, test_acc 93.7\n",
            "step 6000, dt 5.26s, train_loss 3.147e-02, test_loss 4.868e-01, train_acc 98.4, test_acc 94.2\n",
            "step 7000, dt 5.62s, train_loss 1.366e-01, test_loss 7.164e-01, train_acc 98.1, test_acc 92.8\n",
            "step 8000, dt 5.53s, train_loss 4.179e-03, test_loss 5.829e-01, train_acc 99.2, test_acc 93.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(trained_ConvBase_model_results)\n",
        "\n",
        "# batch index  -  test losses - index of a sample\n",
        "trained_ConvBase_model_results[0]['test_acc'][0]\n",
        "\n",
        "# trainset_mask = np.vstack([ret[0] for ret in trained_mlp_model_results])\n",
        "# inv_mask = np.logical_not(trainset_mask)\n",
        "# trainset_correctness = np.vstack([ret[1] for ret in trained_mlp_model_results])\n",
        "# testset_correctness = np.vstack([ret[2] for ret in trained_mlp_model_results])\n",
        "\n",
        "# print(f'Avg test acc = {np.mean(testset_correctness):.4f}')\n",
        "# np.shape(subsets[0]['x'])\n",
        "\n",
        "# len(random_indices)\n",
        "\n",
        "# trainset_mask = np.zeros(train_data_size, dtype=np.bool)\n",
        "# trainset_mask[random_indices] = True\n",
        "\n",
        "# trainset_mask\n",
        "\n",
        "# Create a list of masks that will mark what instances are used in each subset\n",
        "trainset_mask = np.zeros(train_data_size, dtype=np.bool)\n",
        "# trainset_mask[subsets[0]['indices']] = True\n",
        "# trainset_mask[subsets[0]['indices']] \n",
        "# subsets[0]['indices']\n",
        "list_of_mlp_models[0]\n",
        "\n",
        "data_tensor = tf.convert_to_tensor(subsets[0]['x'])\n",
        "data_tensor1 = torch.from_numpy(subsets[0]['x'])\n",
        "data_tensor2 = torch.from_numpy(subsets[0]['y'])\n",
        "\n",
        "# for i in subsets[0]['y']:\n",
        "#   subsets[0]['y']\n",
        "# floats[i] = float(i) for i  in subsets[0]['y'][i] if type(subsets[0]['y'][i]) == Double\n",
        "x_train, x_test = torch.Tensor(subsets[0]['x']), torch.Tensor(subsets[0]['x_test'])\n",
        "y_train, y_test = torch.LongTensor(subsets[0]['y']), torch.LongTensor(subsets[0]['y_test'])\n",
        "  \n",
        "print(np.shape(x_train[0]))\n",
        "\n",
        "# list_of_mlp_models[0] = list_of_mlp_models[0].to(args.device)\n",
        "trainset_correctness = train.accuracy(list_of_mlp_models[0], x_train,y_train)\n",
        "\n",
        "trainset_correctness\n",
        "\n",
        "len(list_of_mlp_models)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTS5Ahb5n_VU",
        "outputId": "5db54fae-3e9e-45fa-d214-a38e5acacfae"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([40])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memorization"
      ],
      "metadata": {
        "id": "IufTPILkDgHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tqdm - package used to shoe a progress bar when loops executing\n",
        "# tqdm - \"progress\" in arabic and obriviation for  \"Te Quiero DeMaciado\" in Spanish (I love you so much)\n",
        "from tqdm import tqdm \n",
        "from mnist1d import train\n",
        "\n",
        "def estimate_mem_infl(list_of_models, model_results, subset_index, subsets):\n",
        "  '''Computes memorization and influence estimates. Parameters: list of all models and their results as well as the subset index in the list of the subsets.'''\n",
        "\n",
        "  # Create a list of masks that will mark what instances are used in each subset\n",
        "  trainset_mask = np.zeros(train_data_size, dtype=np.bool)\n",
        "  trainset_mask[subsets[subset_index]['indices']]  = True\n",
        "\n",
        "  # An inverse of the mask (instances that are not used in this subset)\n",
        "  inv_mask = np.logical_not(trainset_mask)\n",
        "\n",
        "  # Convert splits into the proper forms\n",
        "  x_train, x_test = torch.Tensor(subsets[subset_index]['x']), torch.Tensor(subsets[subset_index]['x_test'])\n",
        "  y_train, y_test = torch.LongTensor(subsets[subset_index]['y']), torch.LongTensor(subsets[subset_index]['y_test'])\n",
        "\n",
        "  # Accuracy of the training set\n",
        "  # trainset_correctness = train.accuracy(list_of_models[subset_index], subsets[subset_index]['x'],subsets[subset_index]['y'])\n",
        "  trainset_correctness = train.accuracy(list_of_models[subset_index], x_train,y_train)\n",
        "\n",
        "  # Accuracy of the testing set\n",
        "  # testset_correctness = train.accuracy(list_of_models[subset_index], subsets[subset_index]['x_test'],subsets[subset_index]['y_test'])\n",
        "  testset_correctness = train.accuracy(list_of_models[subset_index], x_test,y_test)\n",
        "\n",
        "  # memorization estimate\n",
        "  def memorization_estimate(x, mask, axis=0, esp=1e-10):\n",
        "    return (np.sum(x * mask, axis=axis) / np.maximum(np.sum(mask, axis=axis), esp)).astype(np.float32)\n",
        "\n",
        "  #influence estimate\n",
        "  # def influence_estimate(x, mask, esp=1e-10):\n",
        "  #   x = x.T.astype(np.float32)\n",
        "  #   return (np.matmul(x, mask) / np.maximum(np.sum(mask, axis=0, keepdims=True), esp)).astype(np.float32)\n",
        "\n",
        "  mem_est = memorization_estimate(trainset_correctness, trainset_mask) - memorization_estimate(trainset_correctness, inv_mask)\n",
        "  # infl_est = influence_estimate(testset_correctness, trainset_mask) - influence_estimate(testset_correctness, inv_mask)\n",
        "\n",
        "  # return dict(memorization=mem_est, influence=infl_est)\n",
        "  return dict(memorization=mem_est)\n",
        "\n",
        "estimates = []\n",
        "\n",
        "for subset in tqdm(range(15)):\n",
        "  print()\n",
        "  print(subset)\n",
        "  estimates.append(estimate_mem_infl(list_of_mlp_models, trained_mlp_model_results, subset,subsets ))\n",
        "# estimates = estimate_mem_infl(list_of_mlp_models, trained_mlp_model_results, 1,subsets )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hq5Ign7O3EUt",
        "outputId": "aa537689-5dee-48bf-fdce-2b0428dd4fe7"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/15 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            " 13%|        | 2/15 [00:00<00:00, 19.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0\n",
            "\n",
            "1\n",
            "\n",
            "2\n",
            "\n",
            "3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 47%|     | 7/15 [00:00<00:00, 20.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "4\n",
            "\n",
            "5\n",
            "\n",
            "6\n",
            "\n",
            "7\n",
            "\n",
            "8\n",
            "\n",
            "9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 15/15 [00:00<00:00, 25.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "10\n",
            "\n",
            "11\n",
            "\n",
            "12\n",
            "\n",
            "13\n",
            "\n",
            "14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "estimates"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKOwflRtZVx0",
        "outputId": "5c655cb4-8c13-448b-d692-3194c3cdabb7"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'memorization': 0.0},\n",
              " {'memorization': 0.0},\n",
              " {'memorization': 0.0},\n",
              " {'memorization': 0.0},\n",
              " {'memorization': 0.0},\n",
              " {'memorization': 0.0},\n",
              " {'memorization': 0.0},\n",
              " {'memorization': 0.0},\n",
              " {'memorization': 0.0},\n",
              " {'memorization': 0.0},\n",
              " {'memorization': 0.0},\n",
              " {'memorization': 0.0},\n",
              " {'memorization': 0.0},\n",
              " {'memorization': 0.0},\n",
              " {'memorization': 0.0}]"
            ]
          },
          "metadata": {},
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Histogram of Memorization "
      ],
      "metadata": {
        "id": "U6-93SbEdneF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(3.5, 2.5), dpi=150)\n",
        "# err_ixs = np.where(guesses!=trues)\n",
        "# err_classes = trues[err_ixs]\n",
        "\n",
        "# Count occurrence of element '3' in numpy array\n",
        "frequencies = {}\n",
        "for element in estimates:\n",
        "  if element not in list(frequencies.keys()):\n",
        "    frequencies[str(element)] = 1\n",
        "  else:\n",
        "    frequencies[str(element)] += 1\n",
        "    \n",
        "# x = list(frequencies.keys())\n",
        "y = list(frequencies.values())\n",
        "x_ = [i for i, _ in enumerate(frequencies)]\n",
        "\n",
        "print(y)\n",
        "\n",
        "values = [frequencies[i] for i in (frequencies)]\n",
        "\n",
        "plt.bar(x, y, color='green')\n",
        "plt.xlabel(\"Memorizaztion Value\")\n",
        "plt.ylabel(\"freequency\")\n",
        "plt.title(\"Memorization\")\n",
        "\n",
        "plt.xticks(x, y)\n",
        "\n",
        "plt.tight_layout() ; plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "NTpTzp_OdoFU",
        "outputId": "c5d97a9f-908b-42d2-a6e7-0b0ae060415c"
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 525x375 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Showing Examples"
      ],
      "metadata": {
        "id": "t8AanapxJBQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_examples(estimates, n_show=10):\n",
        "  def show_image(ax, image, vmin=None, vmax=None, title=None):\n",
        "    if image.ndim == 3 and image.shape[2] == 1:\n",
        "      image = image.reshape((image.shape[0], image.shape[1]))\n",
        "    ax.axis('off')\n",
        "    ax.imshow(image, cmap='gray', vmin=vmin, vmax=vmax)\n",
        "    if title is not None:\n",
        "      ax.set_title(title, fontsize='x-small')\n",
        "\n",
        "  n_show = 10\n",
        "  n_context1 = 4\n",
        "  n_context2 = 5\n",
        "\n",
        "  fig, axs = plt.subplots(nrows=n_show, ncols=n_context1+n_context2+1,\n",
        "                          figsize=(n_context1+n_context2+1, n_show))\n",
        "  idx_sorted = np.argsort(np.max(estimates['influence'], axis=1))[::-1]\n",
        "  for i in range(n_show):\n",
        "    # show test example\n",
        "    idx_tt = idx_sorted[i]\n",
        "    label_tt = data['test_int_labels'][idx_tt]\n",
        "    show_image(axs[i, 0], data['test_byte_images'][idx_tt], \n",
        "               title=f'test,L={label_tt}')\n",
        "\n",
        "    def _show_contexts(idx_list, ax_offset):\n",
        "      for j, idx_tr in enumerate(idx_list):\n",
        "        label_tr = data['train_int_labels'][idx_tr]\n",
        "        infl = estimates['influence'][idx_tt, idx_tr]\n",
        "        show_image(axs[i, j+ax_offset], data['train_byte_images'][idx_tr],\n",
        "                   title=f'tr,L={label_tr},infl={infl:.3f}')\n",
        "\n",
        "    # show training examples with highest influence\n",
        "    idx_sorted_tr = np.argsort(estimates['influence'][idx_tt])[::-1]\n",
        "    _show_contexts(idx_sorted_tr[:n_context1], 1)\n",
        "\n",
        "    # show random training examples from the same class\n",
        "    idx_class = np.nonzero(data['train_int_labels'] == label_tt)[0]\n",
        "    idx_random = np.random.choice(idx_class, size=n_context2, replace=False)\n",
        "    _show_contexts(idx_random, n_context1 + 1)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.savefig('mnist-examples.pdf', bbox_inches='tight')"
      ],
      "metadata": {
        "id": "UusTyw8-1rg3"
      },
      "execution_count": 105,
      "outputs": []
    }
  ]
}